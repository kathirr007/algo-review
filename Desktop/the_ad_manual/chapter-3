Chapter 4 - Sorting and Searching 

- Why is sorting worth so much attention ? There are several reasons:
	> sorting is the basic building block that many other algorithms are built around. By 	understanding sorting, we obtain an amazing amount of power to solve other 			problems. 
	> most of the interesting ideas used in the design of algorithms appear in the 			context of sorting, such as divide-and-conquer, data structure and randomized 		algorithms. 
	> computers have historically spent more time sorting than doing anything else. A 		quarter of all mainframe cycles were spent sorting data. Sorting remains the most 		ubiquitous combinatorial algorithm problem in practice. 
	> sorting is the most thoroughly studied problem in computer science. Literally 		dozens of different algorithms are known, most of which possess some particular 		advantage over all the other algorithms in certain situations. 

4.1 Applications of Sorting
- We will review several sorting algorithms and their complexities over the course of this chapter. But the punch-line is this: clever sorting sorting algorithms exist that run in O(n log n). This is a big improvement over naive O(n^2) sorting algorithms for larger values of n. 
- Many important problems can be reduced to sorting, so we can use our clever O(n log n) algorithms to do work that might otherwise seem to require a quadratic algorithm. An important algorithm design technique is to use sorting as a basic building block, because many other problems become easy once a set of items is sorted. 
- Consider the following applications:
	> searching: binary search tests whether an item is in a dictionary in O(log n) time, 		provided the keys are all sorted. Search preprocessing is perhaps the single most 		important application of sorting. 
	> closest pair - given a set of n numbers, how do you find the pair of number that 		have the smallest difference between them? Once the numbers are sorted, the 		closest pair of numbers must lie next to each other somewhere in sorted order. 		Thus a linear time scan through them completes the job, for a O(n log n) time 			including the sorting.
	> element uniqueness - are there any duplicates in a given set of n items ? This is a 	special case of the closest-pair problem above, where we ask if there is a pair 			separated by a gap of zero. The most efficient algorithm sorts the numbers and 		then does a linear scan though checking all adjacent pairs.
	> frequency distribution - given a set of n items, which element overs the largest 		humber of times in the set?  If the items are sorted, we can sweep from left to right 		and count them, since all identical items will be lumped together during sorting. To 		find out how often an arbitrary element k occurs, look up k using binary search in a 		sorted array of keys. By walking to the left of this point until the first element if not k 	and then going the same to the right, we can count this in O(log n + c) time, where 		c is the number of occurrences of k. Even better, the. Umber of instances of k can 		be found in O(log n) time by using binary search to look for the positions of both k - 	e and k + e (where e is arbitrarily small) and then taking the difference of these 		positions. 
	> selection - what is kth largest item in an array ? If the keys are placed in sorted 		order, the 5th largest can be found in constant time by simply looking at the 6th 		position of the array. In particular, the median element appears in the (n/2)nd 			position in sorted order. 
	> convex hulls - what is the polygon of the smallest area that contains a given set 		of n points in two dimensions? The convex hull is like a rubber band stretched over 		the points in the plane and then released. It compresses to just cover the points. 		The convex hull gives a nice representation of the shape of the points and is an 		important building block for more sophisticated geometric algorithms. But how can 		use sorting to construct the convex hull? Once you have the points sorted by x-		coordinate, the points can be inserted from left to right into the hull. Since the right-	most point is always on the boundary, we know that it will appear in the hull. Adding 	this new right-most may cause others top be deleted, but we can quickly identify 		these points because they lie inside the [polygon formed by adding the new point. 		These points will be neighbors of the previous point we inserted, so they will be 		easy to find and delete. The total time is linear after the sorting has been done. 
- While a few of these problems namely median and selection can be solved in linear time using more sophisticated algorithms, sorting provides quick and easy solutions to all of these problems. It is a rare application where the running time of sorting proves to be the bottleneck, especially a bottleneck that could otherwise been removed more clever algorithmics. Never be afraid to spend time sorting, provided you use an efficient sorting routine. 
- Sorting lies at the heart of many algorithms. Sorting the data is one of the first things any algorithm designer should try in the quest for efficiency. 
- Given an efficient algorithm to determine whether two sets (of size m and n, respectively) are disjoint. Analyze the worst-case complexity in terms of m and n, considering the case where m is substantially smaller than n. At least three algorithms comes to mind, all of which are variants of sorting and searching: 
	> first sort the big set - the big set can be sorted in O(n log n) time. We can now do 	a binary search with each of the m elements in the second, looking to see if it exists 	in the big set. The total time will be O((n+m) log n). 
	> first sort the small set - the small set can be sorted in O(m log m) time. We can 		now do a binary search with each of the element of the n elements in the big sets to 	see if it exists in the small one. The total time will be O((n+m) log m).
	> sort both sets - observe that once the two sets are sorted, we no longer have to 		do binary search to detect a common element. We can compare the smallest 			elements of the two sorted sets, and discard the smaller one if they are not 			identical. By repeating this idea recursively on the now smaller sets, we can test for 	duplication in linear time after sorting. The total cost is O(n log n + m log m + n + 		m).
	> so which of these is the fastest method ? Sorting the small set is the best of these 	options.  
- Note that expected linear time can be achieved by hashing, Build a hash table containing the elements of both sets, and verify that collisions in the same bucket are in fact identical elements. In practice, this may be the best solution.

4.2 Pragmatics of Sorting 
- We have seen many algorithmic applications of sorting and we will see several efficient sorting algorithms. One issue stands between them: in what order do we want our items sorted ? The answer to this basic question are application-specific. Consider the following considerations:
	> increasing, or decreasing order ? - a set of keys S are sorted in ascending order 		when Si <= Si+1 for all 1 <= i < n. They are descending order when Si >= Si + 1 for 		all 1 <= i < n. Different applications call for different orders. 
	> strong just the key or an entire record? - sorting a data set involves maintaining 		the integrity of complex data records. A mailing list of names, addresses and phone 	numbers may be sorted by names as they key field, but it has better retain the 		linkage between names and addresses. Thus, we need to specify which field is the 		key field in any complex record, and understand the full extent of each record. 
	> what should we do with equal keys? Elements with equal key values will all bunch 		together in any total order, but sometimes the relative order among the keys 			matters. You may need secondary keys such as an article size, to resolve ties in a 		meaningful way. Sometimes it is required to leave the items in the same relative 		order as in the original permutation. Sorting algorithms that automatically enforce 		this requirement are called stable. Unfortunately few fast algorithms are stable. 		Stability can be achieved for any sorting algorithm by auditing the initial position as 	a secondary key. Of course we would make no decision about equal key order and 		let the ties fall where they may. But beware, certain efficient sort algorithms can run 	into quadratic performance trouble unless explicitly engineered to deal with large 		numbers of ties. 
	> what about non-numerical data ? Alphabetizing is the sorting text strings. 			Libraries have very complete and complicated rules concerning the relative 			collating sequence of characters and punctuations. 
- The right way to specify such matters to your sorting algorithms is with an application-specific pairwise-element comparison function.
- By abstracting the pairwise ordering decision to such a comparison function, we can implement sorting algorithms independently of such criteria. We simply pass the comparison function in as an argument to the sort procedure. Any reasonable programming language has a built-in sort routine as a library function. You are almost always better off using this than writing your own routine. For example, the standard library for c contains the qsort function for sorting: [code].
- Read this page - 108 - to get an understanding of quicskort. 

4.3 Heapsort: Fast Sorting via Data Structures 
- 
